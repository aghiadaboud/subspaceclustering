\section{Introduction}\raggedbottom

\subsection{Background}
With the growth in the number of data sources and technologies used to register data over time, a large amount of big data are produced nowadays, which then are used for developing researches, solving problems and gaining new knowledge. The process of analyzing data sets and discovering patterns to achieve such goals mentioned above is called data mining. One of data mining tasks that aims to find similar groups and structures in the data is clustering. In fact, clustering is a well researched technique in data science, but as recorded data have become bigger and bigger in time, classical clustering methods often fail to effectively examine and group the data correctly. With large data we mean data with big feature space where each feature can have a range of possible values. Thus, the need for new clustering tools such as subspace clustering methods has grown.

With the help of subspace clustering algorithms, we aim to better process high dimensional data to better extract information contained within it, detect meaningful clusters possibly hidden in subspaces of the complete feature space and efficiently detect overlapping subspace clusters if the algorithm is capable.
 
One important reason for subspace clustering is that large spaces often consist of noisy, irrelevant and highly correlated dimensions that negatively affects the quality of a full-space clustering. Usually, dimensions in a large space are not concurrently relevant for all given clusters and therefore only need to be considered when they are meaningful. Subspace clustering methods work on solving this local feature relevance problem by searching for relevant subsets of features or important correlation of features for clusters, since clusters might also exist in arbitrarily oriented subspaces.
In addition, the more features represent an observation, the more sparse it becomes. This phenomenon is one of various phenomena known as the curse of dimensionality that states how the volume of a space rapidly increases as the dimensionality increases. Making it harder for distance functions to precisely measure similarity between observations in high dimensional feature spaces as objects appear to be equidistant thus dissimilar. For that, clustering high dimensional data is a challenging task. Besides that, partitioning the data can change from one subspace to another, so for a user it would be interesting to see how data objects variously group in different subspaces.


\subsection{Potential Applications} Subspace clustering can be a useful data mining tool in several applications where observations have too many registered values. 
For example, in bioinformatics, when doing gene expression analysis on gene expression data which contains the expression levels of thousands of genes. Briefly explained, an entry in such data describes the measurement of the level at which information from a gene is used to produce final gene products like protein or non-coding RNA. The expression levels can be measured at hundreds of different timestamps, different tissues, different persons or in different environmental conditions. We also know that genes have different functions under different circumstances. In other words, two genes may only behave coordinately in some environment and are otherwise dissimilar. That means trying to cluster genes across all dimensions would yield biologically inaccurate results. Since meaningful clusters of genes can only be found under a condition, subspace clustering seems to be a perfect solution. We hope by applying it to identify co-expressed genes that are functionally related and to use this information as a basis for further researches.
Subspace clustering of gene expression data shows that this approach, unlike classic clustering, can solve information extraction problems besides data grouping problems.   

An interesting benefit of applying subspace clustering on high dimensional financial data would be detecting anomalies in subspaces that otherwise could not be found when clustering the original feature space. That can be done by identifying points having an anomalous behavior compared to other data points. The reason this process could be so useful is its connection to financial fraud detection. Due to external influential factors from the outside-environment, irregularities could appear frequently in financial data in subspaces. Therefore, detecting an outlier does not necessarily indicate a fraud but it would maybe be enough to analyze the outlier and do start some investigations.

In medicine, subspace clustering of clinical data can be a potential application. For example, when discovering a new drug against some disease, data consisting of dozens of clinical parameters of patients that have taken the drug can be collected, such as lab values, diagnosis results of chronic diseases, medications, etc. The goal of subspace-clustering such high dimensional clinical data would be grouping patients based on their similar body’s reactions to the drug. Now besides groups of patients that have responded positively to the drug, there might be clusters of patients with similar irregular values in some clinical parameters, which may be caused by undesired side effects of the medication. In such a case, pharmaceutical scientists might find it necessary to develop a new medicine that fits those patients.

Today, companies collect great amounts of customers’ information, aiming to use the resulting knowledge out of analyzing this information in achieving more effective marketing. A possible application could be customer segmentation, where companies organize customers into segments based on similar characteristics like similar interests or dislikes. The company can then start offering a specific product or service based on what a segment’s members are interested in. Subspace clustering can be used here to create the wanted segments since traditional clustering methods would fail to handle high dimensional customers’ data correctly.
In general, subspace clustering methods can be used as a tool of business intelligence and be applied on any multi-dimensional customers’ data to extract information that helps business mangers in strategic planning or to make data-driven decisions.

\subsection{Alternative Methods} One can think of dimensionality reduction followed by full-space clustering as an alternative method to subspace clustering that helps avoid the effects of the curse of dimensionality when clustering high dimensional data. The assumption here is that high dimensional data contain a lot of redundant information. Therefore, it can be transformed from a high dimensional space into a lower dimensional space without a significant loss of information. We can differentiate between two approaches of dimensionality reduction, which are feature selection and feature projection. The first keeps a relevant subset of the original feature space based on information gain, measurements of accuracy or prediction errors. The second builds a new smaller set of features from functions of the original input features. The  most impactive drawback of dimensionality reduction techniques is being global, which means points will be clustered only in the one new found coordinate system. Therefore, the information of local clusters that can only be found in a relevant subset of features and overlapping clusters which contain points clustered differently in varying combinations of features would be lost. 

Due to this serious drawback, subspace clustering seems to be more promising. However it may be more computationally intensive. This can be explained by the two actual tasks happening in the process of subspace clustering, which are subspace searching and clustering.
 
Another straightforward alternative method would be to cluster the data objects in every possible subspace of the original feature space. It is to mention here that the number of possible subspaces is an exponential growth function of the number of all dimensions. Exactly there are $2^{d}-1$ different axis-aligned subspaces of a space with d dimensions. That means exploring each possible subspace would lead to a drastic runtime complexity of $O(2^{d})$. It would be expensive and impractical to consider all possible subspaces. Subspace clustering algorithms bypass this problem by using heuristic techniques to help filter out irrelevant subspaces and lower the computation time. 

\subsection{Classification of Subspace Clustering Methods} The task of clustering high dimensional data can be done by different approaches. We can distinguish between:
\begin{itemize}
\item Approaches searching for clusters only in axis-parallel subspaces: like subspace clustering, projected clustering, projection-based clustering and hybrid approaches. These approaches may differ in one or many aspects, such as allowing finding overlapping clusters, the used distance functions, used heuristics, the number of generated subspace clusters, ability to handle noise, etc, but they all work on grouping data objects based on similar values in a subset of features. Examples of such algorithms are CLIQUE \citep{10.1145/276305.276314}, SUBCLU \citep{subclu}, MAFIA \citep{DBLP:conf/sdm/NageshGC01} , PROCLUS \citep{10.1145/304181.304188}, FIRES \citep{fires} , P3C \citep{4053068} and ENCLUS \citep{10.1145/312129.312199}.

\item Approaches searching for clusters in axis-parallel subspaces and in special cases of axis-parallel and arbitrarily oriented subspaces: like pattern-based clustering. A common approach is bi-clutsering, which aims to cluster the rows and the columns simultaneously according to coherent patterns in the data matrix. Examples of such algorithms are FLOC \citep{10.1109/ICDE.2002.994771}, MaPle \citep{10.1109/ICDM.2003.1250928} and OP-Cluster \citep{10.1109/ICDM.2003.1250919}.

\item Approaches detecting clusters in axis-parallel and arbitrarily oriented subspaces: like correlation clustering. Such an approach is applied when features are complexly correlated so that data matrices do not contain any particular patterns, so algorithms in this field do not restrict the search to axis-parallel subspaces. As a result, this approach is computationally less efficient than only axis-parallel approaches because there are an infinite number of possible arbitrarily oriented subspaces in a space of d dimensions. Examples of such algorithms are ORCLUS \citep{10.1145/342009.335383}, EriC \citep{DBLP:conf/ssdbm/AchtertBKKZ07}, CASH \citep{DBLP:conf/sdm/AchtertBDKZ08} and COPAC \citep{DBLP:conf/sdm/AchtertBKKZ07}.
\end{itemize}
These various clustering approaches can be further classified based on shared characteristics, such as allowing detecting overlapping clusters and overlapping subspaces, used search methods or cluster model.

With regard to overlapping clusters, we can divide algorithms into two families, namely, algorithms that produce non-overlapping clusters by either assigning each data object to a unique cluster or marking it as a noise and algorithms that take into account the fact that data objects can belong to different clusters in varying subspaces and thereby work on finding all possible cluster in all subspaces of the feature space, which automatically lead to detect overlapping clusters. Examples of overlapping clusters algorithms are CLIQUE, SUBCLU, MAFIA, FIRES and ENCLUS. Examples of non-overlapping clusters algorithms are PROCLUS and PreDeCon \citep{DBLP:conf/icdm/BohmKKK04}.

When considering the used search methods, diverse approaches can be categorized into two groups depending on their methodology for navigation through the search space of possible subspaces. The first group includes methods that pursue a bottom-up approach, where methods of the second group pursue a top-down approach. 

Bottom-up approaches first begin with identifying all relevant 1-dimensional subspaces, then they combine those to build higher-dimensional subspaces and explore them. Detecting relevant 1-dimensional could be achieved by first applying a traditional clustering method to each 1-dimensional subspace. Secondly, keeping all 1-dimensional subspaces in which at least 1-dimensional cluster exist. 

This procedure can be explained by the monotonicity property used by many algorithms, which assumes that if a cluster exists in a subspace then this cluster also exists in all lower-dimensional projections of this subspace. To prune irrelevant dimensions, algorithms actually use the inversion of this assumption, which is as follows: if a dimension contains no clusters, then no higher-dimensional subspace of this dimension contains a cluster. This monotonicity assumption helps keep the bottom-up search efficient at the risk of producing lower quality results.

The combining step of subspaces is done as follows. Algorithms generate iteratively higher-dimensional subspaces by increasing the dimensionality by 1, so 1-dimensional subspaces are combined to build 2-dimensional ones, then 2-dimensional ones are combined to build 3-dimensional ones and so on, until no more potential higher-dimensional subspaces can be built. A special case of the bottom-up approach can be seen in FIRES \citep{fires} where 1-dimensional subspaces are combined to directly generate subspaces of maximum dimensionality.

Some algorithms do not generate high-dimensional subspaces by simply building combinations of lower-dimensional ones, instead they adopt a merging procedure to decide whether to combine subspaces or not. For example, by measuring the cardinality of the intersection of clusters in the related subspaces. Some examples of bottom-up algorithms are CLIQUE, SUBCLU, MAFIA, ENCLUS and P3C.
 
While on the contrary, top-down approaches start with detecting clusters in the full dimensional space and then iteratively learning the best subspace for each cluster by removing irrelevant dimensions. This is a cluster-based approach. An iterative instance-based approach starts in the full dimensional space with learning the preferential subspace for each data point, which is the subspace containing the best-fitting cluster for the point. Subsequently, points having similar preferential subspace are grouped together to form clusters. Most top-down algorithms compute a global discrete partitioning of the data objects, thus they do not allow overlapping clusters. Examples of such approaches are PROCLUS and COSA \citep{RePEc:bla:jorssb:v:66:y:2004:i:4:p:815-849}.

Approaches for clustering high-dimensional data can also be characterized by their cluster model into grid-based (also called cell-based) or density-based approaches.

The general idea of grid-based clustering approaches, also called cell-based approaches, is to partition a data space by an axis-aligned grid into a finite number of disjoint cells, then to determine dense cells and connect them to form clusters. Dense cells are grid cells containing a number of objects above a certain threshold and clusters are maximal sets of connected neighboring dense cells within a subspace. This procedure is done recursively during the navigation through the search space of possible subspaces.

Grid-based approaches can be further classified into static grid and adaptive grid approaches. Static grid approaches discretize the data space into equal sized cells all having the same width. CLIQUE is an example of such an algorithm. On the other hand, adaptive grid approaches arbitrarily partition a data space into variable sized cells, to allow maximizing the number of objects within particular cells. The algorithm MAFIA (Merging of Adaptive Finite IntervAls) is an example.

Grid-based approaches have a major drawback caused by the impact of the positioning of the grid. For example, some points on the edge of a cluster might be missed if they are not within dense cells or some points might be wrongly assigned to a cluster just because they exist in related dense cells, even though they are not actual cluster members. So the accuracy of the clustering could be negatively affected by the orientation and the shape of the clusters in relation to the position of the grid.

Density-based approaches are often more complex but capable of detecting clusters of any size and shape, thus more accurate. These approaches are based on the density-connected cluster paradigm introduced in DBSCAN \citep{10.5555/3001460.3001507}. By exploring the neighborhood of points and identifying core points in a subspace, these approaches detect regions of high density that are separated by regions of lower density. A cluster is defined as a maximal set of connected dense points. Examples of such algorithms are SUBCLU and FIRES.

Finally, subspace clustering approaches could be clustering-based regardless of whether they are grid-based or density-based, bottom-up or top-down. Such algorithms use global parameters to define properties of the expected clusters, like the number of clusters or the average dimensionality of these clusters. Examples of such algorithms are PROCLUS and P3C.

\subsection{Main Topic of Project} Now that we have basic knowledge about the various characteristics of approaches for clustering high-dimensional data and the different techniques and paradigms used by these approaches, we can turn our focus towards the main purposes of writing this modest thesis which is presenting an implementation of SUBCLU (density-connected Subspace Clustering) and FIRES (Filter Refinement Subspace clustering) in Python, two subspace clustering algorithms used to cluster high dimensional data. We will also compare both in many aspects and in different scenarios.

With these implementations we aim to help researchers by further studies in relation to SUBCLU and FIRES. Also, with some modifications, both implementations could be adapted to match the style of the pyclustering library (a Python, C++ data mining library) \citep{Novikov2019} and thus could be added to the list of implemented algorithms since SUBCLU and FIRES are yet not in the library.
